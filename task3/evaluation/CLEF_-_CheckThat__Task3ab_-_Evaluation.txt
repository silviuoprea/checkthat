{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import re, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from tkinter import Tk, filedialog\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = [\"1a46b040\", \"c1e124d5\", \"8209ca7b\", \"e2abfbe6\", \"39f5c37f\",\n",
    "              \"47423bb6\", \"097c142a\", \"08bc59f4\", \"af3393ce\", \"a39d07df\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Tk()\n",
    "root.withdraw() #dont show base window\n",
    "path_to_test_data = filedialog.askopenfile() #file selection dialog\n",
    "print(path_to_test_data.name) #check, that the correct file has been selected\n",
    "\n",
    "#depending on whether task a or task b testing data has been loaded\n",
    "column_name_test = \"our rating\" if \"3a\" in path_to_test_data.name else \"category\"\n",
    "column_name_system = \"predicted_rating\" if \"3a\" in path_to_test_data.name else \"predicted_domain\"\n",
    "\n",
    "#read gold standard\n",
    "#If you don't wan't to use a dialog, specify the path directly\n",
    "test_data = pd.read_csv(path_to_test_data.name, encoding=\"UTF-8\")\n",
    "test_data.head() #quick check, if the data has been correctly read. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participants file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read participant data \n",
    "\n",
    "#If you don't wan't to use a dialog, specify the path directly\n",
    "path_to_system_predictions = filedialog.askopenfile() #file selection dialog\n",
    "print(path_to_system_predictions.name) #check, if the correct file has been selected\n",
    "\n",
    "if path_to_system_predictions.name.endswith(\"tsv\"):\n",
    "    system_predictions = pd.read_csv(path_to_system_predictions.name, encoding=\"utf-8\", sep=\"\\t\") \n",
    "else:\n",
    "    system_predictions = pd.read_csv(path_to_system_predictions.name, encoding=\"utf-8\")\n",
    "\n",
    "#Check, if the number of entries is correct\n",
    "print(len(system_predictions))\n",
    "system_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only for 3A\n",
    "system_predictions_no_duplicates = system_predictions[~system_predictions[\"public_id\"].isin(duplicates)]\n",
    "if(len(system_predictions) - len(system_predictions_no_duplicates)==10):\n",
    "    print(\"Dropping duplicates was successful\") \n",
    "    system_predictions = system_predictions_no_duplicates.reset_index(drop=True)\n",
    "else:\n",
    "    print(\"Dropping duplicates was NOT successful. Please mark for a later check.\")\n",
    "    print(f\"system_predictions has {len(system_predictions)} entries and system_predictions_no_duplicates has {len(system_predictions_no_duplicates)} entries.\")\n",
    "    \n",
    "print(f\"system_predictions has {len(system_predictions)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check public ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check, if the id columns are identical\n",
    "#there are some public ids, that are saved differently depending on the editor used to view the file \n",
    "#    test_data    system_predictions (example for public ids that might be problematic)\n",
    "#    public_id    public_id\n",
    "#86   080e0024       8E+025\n",
    "#129  17334e19  1,7334E+023\n",
    "#210  84231e18  8,4231E+022\n",
    "#219  85100e77    8,51E+081\n",
    "#271  04866616      4866616\n",
    "#272  03963580      3963580\n",
    "#363  02695016      2695016\n",
    "#If you can verify that the ids are identical, than you can proceed with the evaluation. If not, please inspect further.\n",
    "print(test_data[\"public_id\"].isin(system_predictions[\"public_id\"]).value_counts())\n",
    "matched = pd.DataFrame(test_data[\"public_id\"].eq(system_predictions[\"public_id\"]))\n",
    "if len(matched[matched[\"public_id\"]==False]) == 0:\n",
    "    display(Markdown(\"<span style='color: #ff0000'>everything is fine, please proceed with the evaluation.</span>\".upper()))\n",
    "else:\n",
    "    try:\n",
    "        print(pd.concat((\n",
    "            test_data[\"public_id\"].loc[\n",
    "                test_data[\"public_id\"].where(test_data[\"public_id\"]==system_predictions[\"public_id\"]).isna()], \n",
    "            system_predictions[\"public_id\"].loc[\n",
    "                system_predictions[\"public_id\"].where(system_predictions[\"public_id\"]==test_data[\"public_id\"]).isna()]), axis =1))\n",
    "    except ValueError as e:\n",
    "        display(Markdown(\"<span style='color: #ff0000'>The number of entries in both files is different. An evaluation is not possible.</span>\".upper())) \n",
    "        merged_dataframe = pd.merge(test_data[\"public_id\"], system_predictions[\"public_id\"], how=\"outer\", indicator=True)\n",
    "        print(\"entries missing in the participants file:\\n\", \n",
    "              merged_dataframe[merged_dataframe[\"_merge\"]==\"left_only\"][\"public_id\"])\n",
    "        print(\"\\nentries missing in the systems file:\\n\", system_predictions[\"public_id\"].loc[\n",
    "        system_predictions[\"public_id\"].isin(merged_dataframe[merged_dataframe[\"_merge\"]==\"right_only\"][\"public_id\"])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    classification_report_results = classification_report(test_data[column_name_test], system_predictions[column_name_system].str.lower().str.strip(), output_dict=True)\n",
    "    eval_results = pd.DataFrame.from_dict(classification_report_results).transpose()\n",
    "    participant_file = re.sub(\".+/(\\d+)/*[\\w_-]*/([\\w_-]+)(\\.(tsv|csv))?\", \"\\\\1_\\\\2\", path_to_system_predictions.name)\n",
    "\n",
    "    eval_results.to_csv(\"classification_report_\" + participant_file + \".csv\")\n",
    "    #pretty print\n",
    "    print(classification_report(test_data[column_name_test], system_predictions[column_name_system].str.lower().str.strip(), digits=4))\n",
    "except KeyError as e:\n",
    "    print(\"The column names of the participant file don't match the required column names ('public_id, 'predicted_rating'):\", \n",
    "          system_predictions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Destroy the \"invisible\" root window\n",
    "root.destroy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
